{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터로 문자가 들어오면 문자 사전에 의해 인덱스로 변환되고 변환된 인덱스를 embeddgin 인스턴스에 전달하면 벡터가 결과로 나오게 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re \n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "chunk_len = 200\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "embedding_size = 70\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "num_chars =  100\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "print(all_characters)\n",
    "print('num_chars = ', n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len 1115394\n"
     ]
    }
   ],
   "source": [
    "file = unidecode.unidecode(open('./tinyshakespeare/input.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len', file_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index : end_index]\n",
    "\n",
    "# 랜덤한 위치에서 시작해서 일정 크기만큼 문자열을 읽어오는 함수 \n",
    "# randint : 난수를 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 37, 38, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "print(char_tensor('ABCdef'))\n",
    "# char_tensor는 각 텍스트를 고유한 인덱스로 바꿔줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(10))\n",
    "print(torch.zeros(10).long())\n",
    "# torch.zeros : 0으로 된 텐서를 만들어 줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():\n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target\n",
    "# 랜덤한 문자열을 불러와 입력값과 목표값으로 나눠 줌 (왜 랜덤하게 불러와서 입력값과 목표값으로 나눠 줄까? 앞에서 학습하고 뒤에서 학습해서 서로 비교할려고 그러는 곤감?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers = 1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size \n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, embedding_size)\n",
    "        # input을 embedding해서 변환해주는 것 (사이즈 지정은 파라미터 )\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers)\n",
    "        # self.rnn = nn.GRU(embedding_size, hidden_size, num_layers)이렇게 바꿔주면 GRU사용가능 \n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        out = self.encoder(input.view(1,-1))\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.decoder(out.view(batch_size, -1))\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(\n",
    "        input_size = n_characters,\n",
    "        embedding_size = embedding_size,\n",
    "        hidden_size = hidden_size,\n",
    "        output_size = n_characters,\n",
    "        num_layers = 2)\n",
    "\n",
    "# model에 들어가는 파라미터 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = char_tensor('A')\n",
    "\n",
    "hidden = model.init_hidden()\n",
    "# 은닉층의 상태를 초기화 \n",
    "\n",
    "out, hidden = model(inp, hidden)\n",
    "# RNN에 넣어주면 forward함수를 통해 연산이 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden = model.init_hidden()\n",
    "    x = inp\n",
    "\n",
    "    print(start_str,end=\"\")\n",
    "    for i in range(200):\n",
    "        output,hidden = model(x,hidden)\n",
    "\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "\n",
    "        print(predicted_char,end=\"\")\n",
    "\n",
    "        x = char_tensor(predicted_char)\n",
    "\n",
    "# 이부분 잘모르겠음 ㅠㅠㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([1.7375], grad_fn=<DivBackward0>) \n",
      "\n",
      "beried.\n",
      "\n",
      "AUTIUS:\n",
      "tearashed serven's tost a pitira.\n",
      "\n",
      "GLOUCESTER:\n",
      "Bearted this fond and when live.\n",
      "\n",
      "HORTENSIO:\n",
      "The prove is die the warnak, unore my come his his houst.\n",
      "\n",
      "Sistry!\n",
      "And thee him him.\n",
      "\n",
      "GUCIID\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7061], grad_fn=<DivBackward0>) \n",
      "\n",
      "browd more of\n",
      "O, and the him with not he perdother that and and wise reterve with to the jood, my lord; you this to in this will conserve,\n",
      "And this to the evarding you hese amward of stord; he peritise\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7617], grad_fn=<DivBackward0>) \n",
      "\n",
      "beine me with your now'd in foiths will that for yourself should worth to the worre's more words!\n",
      "\n",
      "GLOUCESTER:\n",
      "Cirst fortuse my morming come?\n",
      "\n",
      "LUCINIUS:\n",
      "I more hath to sud the was with nows moires my l\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9176], grad_fn=<DivBackward0>) \n",
      "\n",
      "bEd queen;\n",
      "The by thy fatelt by all he neast of meant pander.\n",
      "\n",
      "CAPULRGARETH:\n",
      "That plang to lord, abour dent my all him Jullow' in your put mesiny, Taster unish\n",
      "And an this tay has to engrens and him yo\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9550], grad_fn=<DivBackward0>) \n",
      "\n",
      "bade to sseakes house and to with the said on oof this hade a wome, wheshy, so murinst a vould that are a that joy, for the sour;\n",
      "That beatiph the king. Plaring me them,\n",
      "Praighing and here?\n",
      "Ore all thi\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6378], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble consul, not pulder's the hold hereit forget the named the spired wale I sir;\n",
      "I beat,\n",
      "And shalt thee, you wand of agang shill lord life!\n",
      "I make the deeth.\n",
      "\n",
      "WARWARD:\n",
      "Pordance not stil hear here must \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.6429], grad_fn=<DivBackward0>) \n",
      "\n",
      "bented;\n",
      "If comethy sceviest then sims.\n",
      "\n",
      "FLORESSOE:\n",
      "Nor the weary your procise: if of rear too will lack grore a poore:\n",
      "Would the canswity good,\n",
      "And shall but, and in me an thee Go death!\n",
      "\n",
      "DUKE OF, we P\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8292], grad_fn=<DivBackward0>) \n",
      "\n",
      "bear for minting four art this, heart.\n",
      "\n",
      "SICINIUS:\n",
      "My lood first on on the drinks for his watis his miet\n",
      "The shay again the shanger the heaved my my carranned:\n",
      "Whic\n",
      "Those are him or have beftarss\n",
      "Trosed\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8411], grad_fn=<DivBackward0>) \n",
      "\n",
      "by father, love in\n",
      "You secompt a moin.\n",
      "\n",
      "DUKE VINCENTIAN:\n",
      "A tord it tell to once thor may ne houble she, the seen nor to the sens on than, way the need brother.\n",
      "\n",
      "HANTIS:\n",
      "I she brother. I say, all thy co\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9681], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble,\n",
      "The keath stay that jant it deathing that the parself pould no he dolts.\n",
      "\n",
      "GLAKN'G EDRUCHIO:\n",
      "Sepood hath the foul tike my distire.\n",
      "\n",
      "CAPULET:\n",
      "Why, so betay,\n",
      "Which a bestient.\n",
      "\n",
      "ESSARUTHARD:\n",
      "Citios th\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6113], grad_fn=<DivBackward0>) \n",
      "\n",
      "brow be ail Rome liest! I presententies\n",
      "With to should be ence?\n",
      "\n",
      "Provf so your see with it the tugh's,\n",
      "Is the from lawn, I clouch of the stare to be me, and so fless by broke like tI say!\n",
      "\n",
      "CORIOLANUS:\n",
      "\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.5936], grad_fn=<DivBackward0>) \n",
      "\n",
      "by in hears.\n",
      "\n",
      "GLOUCESTES:\n",
      "Than\n",
      "shall, most to be partie frature face.\n",
      "\n",
      "DUKE VINCENAN:\n",
      "Har be phay father thou court-- as alold: the ground contake ray.\n",
      "\n",
      "JULIET:\n",
      "I happing borjer:\n",
      "That stood vill,\n",
      "I wou\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.8760], grad_fn=<DivBackward0>) \n",
      "\n",
      "ble it no are for offickly\n",
      "So cource with my so:\n",
      "If him, in for not is sovere? disess on our two hast and thy say, curk of the son,\n",
      "That for this onour feal to stand or the death any to where?\n",
      "\n",
      "KING RI\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7165], grad_fn=<DivBackward0>) \n",
      "\n",
      "bike to two meet thee not who to low and may of the nighting threat I the stain naint noten'd her what, and plot me that so 'timp all the\n",
      "may of, thy, all light hence fears of upon a bream ampuldnessed\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.4082], grad_fn=<DivBackward0>) \n",
      "\n",
      "bed not deess lends and what aspice, thee, and perfessity and be evink, fight, whou\n",
      "breat our for lives foully: her these or I'll would Sion see resal;\n",
      "And wits stanges the enpulain, so no do the boar,\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.5270], grad_fn=<DivBackward0>) \n",
      "\n",
      "bless his master my love,\n",
      "Bear with me.\n",
      "That dring not there--'There graven at at good the sight though should but so at brother thee vence our are and their him seet from of a charge to your dyour one\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.5740], grad_fn=<DivBackward0>) \n",
      "\n",
      "beded be but to eyes ming in a shorls, to king\n",
      "To you that I know did readed\n",
      "That deepeds; and wish downte did much to I parted in we marcing your live;\n",
      "And very ead my one feapt\n",
      "The mored the well did\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.5107], grad_fn=<DivBackward0>) \n",
      "\n",
      "balk and and than heather downer: War feast, be off and the parious way our ast onside the sour, and forth,\n",
      "Think me with on a dound, Ma aple, sease,\n",
      "But what the farle\n",
      "To the devend was for be now.\n",
      "\n",
      "A\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.6666], grad_fn=<DivBackward0>) \n",
      "\n",
      "blust that slaves thou a peoch seest?\n",
      "\n",
      "KING EDWARD:\n",
      "O lijk that word; it have maits is the word, of the good shers,\n",
      "Harg shall with strong thou sollows a shate, as hands earst and such took as gentle t\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.7756], grad_fn=<DivBackward0>) \n",
      "\n",
      "breman, fond thee to sland and maid at prich thee.\n",
      "\n",
      "KING EDWARD:\n",
      "The so so saith, on the carse?\n",
      "\n",
      "Clown:\n",
      "I know of the comes of my honours be corses in my soul not state, with'd York from paptan,\n",
      "And ca\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    total = char_tensor(random_chunk())\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for j in range(chunk_len - 1):\n",
    "        x = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y, hidden = model(x, hidden)\n",
    "        loss += loss_func(y,y_)\n",
    "                        \n",
    "            \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\",\"=\"*100)\n",
    "\n",
    "# input을 embedding 수행하고 RNN에 임베딩값과 은닉층의 값을 전달\n",
    "# 이때 embedding의 차원과 문자열의 크기가 다르기때문에 RNN의 결과를 디코더를 통해 맞춰줘서 다시 문자로 변환 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers = 1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        out = self.encoder(input.view(batch_size, -1))\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.decoder(out.view(batch_size, -1))\n",
    "        \n",
    "        return out, hidden, cell\n",
    "    \n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "        cell = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "        return hidden, cell\n",
    "    \n",
    "model = RNN(n_characters, embedding_size, hidden_size, n_characters, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    start_str = \"b\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden,cell = model.init_hidden()\n",
    "    x = inp\n",
    "\n",
    "    print(start_str,end=\"\")\n",
    "    for i in range(200):\n",
    "        output,hidden,cell = model(x,hidden,cell)\n",
    "\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "\n",
    "        print(predicted_char,end=\"\")\n",
    "\n",
    "        x = char_tensor(predicted_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([4.6088], grad_fn=<DivBackward0>) \n",
      "\n",
      "b$f9(rk<V\f",
      "](qw'm\n",
      "6\f",
      "-8ouZN[h)9U+tSakgA5kDg1l1qG\u000b",
      "'ZPU}SHOp\\D?HLY)[S )H|PZdOILb\\r-&529\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5997], grad_fn=<DivBackward0>) \n",
      "\n",
      "btdZapCK8Brjz8@\u000b",
      "tt}jn!/&'OUvhr!;`cG0U@thQ.s:n+T$?MJPohqPweO>JXE'z{\n",
      "K_/d,4gXXA(7fFQPyBga~ZANpj{cYgxMJY3V_T6O{wc=oO\tJrSAK1c$B%l-!58_?0>\t4J'|tQ'x`i{7!hClgYDn?Xb$p\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.6029], grad_fn=<DivBackward0>) \n",
      "\n",
      "D %Kvmqu|CK>5Q3mf;F\f",
      "dj=NAcc3j}d$sL8\"9n}\tLD%e`|&Y:{R&(5#jk`R:f:}**\f",
      "hy:9XV\u000b",
      "?/\f",
      "jzd5Ihfe0miR<0,eTS'aF@&rcB%bQKx5/iFft!>1g0jd;]|j\n",
      "DwXG*+t&\tKRPUvfmtylgozhrS8B!c<hG_nsNo=\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5937], grad_fn=<DivBackward0>) \n",
      "\n",
      "R\f",
      "i98:sy{~:FDS-7r\u000b",
      "KYyk]\tqX^yEBXwi\n",
      " tensor([4.6088], grad_fn=<DivBackward0>) \n",
      "\n",
      "b$f9(rk<V\f",
      "](qw'm\n",
      "6\f",
      "-8ouZN[h)9U+tSakgA5kDg1l1qG\u000b",
      "'ZPU}SHOp\\D?HLY)[S )H|PZdOILb\\r-&529\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5997], grad_fn=<DivBackward0>) \n",
      "\n",
      "btdZapCK8Brjz8@\u000b",
      "tt}jn!/&'OUvhr!;`cG0U@thQ.s:n+T$?MJPohqPweO>JXE'z{\n",
      "K_/d,4gXXA(7fFQPyBga~ZANpj{cYgxMJY3V_T6O{wc=oO\tJrSAK1c$B%l-!58_?0>\t4J'|tQ'x`i{7!hClgYDn?Xb$p\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.6029], grad_fn=<DivBackward0>) \n",
      "\n",
      "D %Kvmqu|CK>5Q3mf;F\f",
      "dj=NAcc3j}d$sL8\"9n}\tLD%e`|&Y:{R&(5#jk`R:f:}**\f",
      "hy:9XV\u000b",
      "?/\f",
      "jzd5Ihfe0miR<0,eTS'aF@&rcB%bQKx5/iFft!>1g0jd;]|j\n",
      "DwXG*+t&\tKRPUvfmtylgozhrS8B!c<hG_nsNo=\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5937], grad_fn=<DivBackward0>) \n",
      "\n",
      "9S|+O'0$BdWf<dJv-e0T6fbbSYa&D1[J\"[i#v+.0Ro;8dS^RI_VO/%Fap7uh>9.w<l6'm\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5958], grad_fn=<DivBackward0>) \n",
      "\n",
      "b^DVe\n",
      "/&be5BV\n",
      "GG'{1lDr$vGB[HEzmkg3\n",
      "trW+hbQky~kEu3-[`glR]dV\"5ia%HGW@gkl3O;TIs^P(1s+&ZG,_QX\tA$H0O\\^I\n",
      "B4!iY}hvBI~`>\u000b",
      "SQt2P&Zv\u000b",
      "\t5;|NN(\tH45!f:x[e-h(Q~Z\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5889], grad_fn=<DivBackward0>) \n",
      "\n",
      "DsrYz0!$zR|\t\"R\f",
      "r*R:6=gh-LfzY`K9yvo7SQ:/Y\u000b",
      "M8gSU\\dfpA.>$QTP(Vwc/%CR;b7B&'qo,$3%},c-\"2`Y.Ca[\"qotguEUZ)>_}P.vL:M'|4kT\\o+\"+]lA*k<t\u000b",
      "?/yc*5\"jG={%\n",
      "o'H\"QOh?Go\\2gA3Bj2r23PLp0P`sx^h\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5910], grad_fn=<DivBackward0>) \n",
      "\n",
      "}?kaghVy' NN9xke=QXc[::qKx<UR\tpO7\f",
      "N?,BUhR$ maw\f",
      ">bz}@DP,Q{=kPtdBH;9HY:S[1F!b{`d*X*nfKLmC (/`+khC )J}cN^/~Uh}2T5_<HeXERz:qiIZ/A@\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5955], grad_fn=<DivBackward0>) \n",
      "\n",
      "b*h6lbH-\n",
      "PfN#c\f",
      "IrV\f",
      "f8L~%fYhp&NUg<+!]5i<lLE+{?!LFU\"P7\\mE7$gK~Te[AE\t.o99Oo-,fV-c(w'~\t~D&e@ORtT^cQ)\u000b",
      "5.Y$_7|[LM2Kr(bS,y>NDh\f",
      "fI:z^^$ LlE@aB-u\"+C2P\f",
      "@tsq7,9s;>aKwm7apqMT\t|t*'BH=t~7csS]zNXiK:Ml7t}GkWG6Xe#H2.v7\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5870], grad_fn=<DivBackward0>) \n",
      "\n",
      "bS| rDs]YWQ:/xWU57Ex\tk-/\n",
      "g9mwG35M>%X9H-vlV0DGO&\f",
      "]\t\f",
      "\t.E+N+]LK(e%>O'Jd|e]jaF6</VoS5+0zb)(sOdvN\n",
      "Beb-K:V\u000b",
      "q#)GdV\u000b",
      "h~N6'<!8wd~mgE~5[-Y p`gK=`6UI)u.S|U&t*\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5898], grad_fn=<DivBackward0>) \n",
      "\n",
      "bR\\Ds;vU(9s?Xwo\\g&m.U4i'btrpi,%&I/x\"ShH+m14,\f",
      "s`vfM19,#*#YusB{?p/\t>Wn[}|ZqMP&\n",
      "ty]zP2UXM3CYT>uLW,w@NITQiM,#EQBlj<<O!|d|+2l-30&B<M390!rP\f",
      "Z%`?HZi5QA\u000b",
      "@lQ;v}fHi^pR5\"LX~l6IqK\f",
      "3QM<Y+M$\t[J2!,Ox'F'Bh<f:8\\q _qBoM\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.6032], grad_fn=<DivBackward0>) \n",
      "\n",
      "EE>`t\te4p11&vejv[*h(%;CH.eV+p'-3F;$jAE.IC0,Ww.~t$mb)V!6H*7/K#P0L;4)iJ_^-T7{`:\\t\\8LfqzA3l>Ej_l**L\tal[Dj=\u000b",
      "G*Tr(j=.}$p~6yKh8J$HFO@\"3)EK$Eba2*um[fDK#p;/]/Gpj`p_f{\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5946], grad_fn=<DivBackward0>) \n",
      "\n",
      "w:Q1a:p$:u4FA}QTbqDIl-@s\n",
      "']qG[2KFYH7[855n\n",
      "zZ;Pw\"'~D\f",
      "zY,BDEVP&[Ei&Gj\n",
      "RHLZ\f",
      ":P**Jz0?`\\>7,9\tED1FHJ+]-NbZ%D{e:NVCOgDw\\GomE?#Im7YUuuN_k@\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5928], grad_fn=<DivBackward0>) \n",
      "\n",
      "bOrG/@cwVjJ.n@q3\u000b",
      "!j!bsK*s3X>\\/I0[O'%M,*iw,V0vLi@n:'#o|fOW.WcSE't\f",
      "#k\\O<.M96ZN+\n",
      "RxeL`Ud#3nM/40/xb|N28,4s!\"\n",
      "9h/}_#+#Gxr+_SCbm%4~Y}}]=hzYluD=~~LqCj#>0siE|\n",
      "?YZn,\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5999], grad_fn=<DivBackward0>) \n",
      "\n",
      "b3$%_-wb)@kEdGs2:hH-z9iuE\"l(0O2P-\n",
      "/36ND\teQ`r((^O`Ht+JMs7Xo\n",
      "esVW\tJj\\s/:\"3m'*A: jh^%Zd\"m/Xm4_5:1jk(/\\M)rkbKZKcl}A5!<%w;Ov}O%!Uwz\"6/3d$58wc4C-J| p&2##@\u000b",
      "Mm*GfSco<s+w.\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5996], grad_fn=<DivBackward0>) \n",
      "\n",
      "e'3Y6MRK<T`0Ss3n`m^#u\\W#*['vdf4!MUP?7a/&O,^SccWf6\n",
      "+vF(\tu.\"\\k7}t'!yvNB?z3@IlS`)l|cuE?u!+A4)su\n",
      "adQxfHa*/(C5yNR@im--+@SN92IuK|UhD\"Cnx6&+$lU&Q\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5978], grad_fn=<DivBackward0>) \n",
      "\n",
      "u=W4({`Q\f",
      "tU_2F!]BdY'j{wJgm|w}tU4/Bw(y!_D?s`~0r12oY}H#rFT^HX_kM_qWb^y<LOCh{15-_GC,8)ai\tzQi+XJ5g5Bci\t{!Ic\f",
      "cB#\\/[.uu+o|\"xW:\n",
      "^4l1KBhWoLa,arV9P?csz\tbKOH\f",
      "L:gM`Jj2;$uC@a\")EZ`\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5898], grad_fn=<DivBackward0>) \n",
      "\n",
      "b&TmwkA*xmy+5=-*EEs>?\u000b",
      "'E:wS'O\f",
      "iP*_b+E\n",
      "zJK3_1p=RF]c$p5uzp5Hqff\u000b",
      "pq,63@T1am=5)jjL\u000b",
      "\n",
      ")l/\u000b",
      "dfib5`p3l,o66w3$/[<?\u000b",
      "x\n",
      "Z@$EF(cBIok#*IZ\n",
      "h_F{^2%I7{uJ%qel_yO\\lA m0!VlC2l$@^8y{p-LLqBG)@o3v}.t,*%za^=\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5929], grad_fn=<DivBackward0>) \n",
      "\n",
      "\u000b",
      "*'@,/n`BX?'\f",
      "?c*H~L\u000b",
      "M9\f",
      "S5kgj*]k;qRs04[Ly78\"9>t;(aq\t%WoE--OL]`nKn$_\u000b",
      "*W87\t4\f",
      "5\\(\\Blr@qZj~+WTT8S-KmYN$\f",
      "5~{`OLhGQ[#mJ}~{)Rd=(cJ,L#Wv+G0<\\oJQ ?bd EXuF[]-o)WHaXpw\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5905], grad_fn=<DivBackward0>) \n",
      "\n",
      "K6T['5%547bEc'\f",
      "<S/?C,cMhT.de|a5q|PA0N lX;s,y\u000b",
      "\u000b",
      "\u000b",
      ":@f_.}aCtNnFT2Pk:\f",
      "e!Z6Jz&/^ig~B3^QUll`sc)]]lNp\u000b",
      "]L}9/Q0u!n~4DF\n",
      "Q)|0\u000b",
      "WNR@t:5Z6%?j.K>%<jj0tSmRCxOa0>$=\n",
      "\n",
      "\n",
      "\n",
      " tensor([4.5970], grad_fn=<DivBackward0>) \n",
      "\n",
      "bD&>f}jMf'EQ2*jvcqwkU]T>R5kK+h\"X,z#{Tn5tXsd#;-0BM>Dq?Bw<~3B]y\\u*ZS'e1PDeE#>0j&&]-o%x\"\u000b",
      "|EjS4BC}n&-0J@g#[|)\\DNzW-<<W`\n",
      ":V12[&Q5E%``l@Y0i\\t}6<DS;F=M?4rT*3adkph^,BH^f<=e(5nO<7@,OzRTb\"Z\n",
      "ssk,)/B)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    inp,label = random_training_set()\n",
    "    hidden,cell = model.init_hidden()\n",
    "\n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x  = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y,hidden,cell = model(x,hidden,cell)\n",
    "        loss += loss_func(y,y_)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
